{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opzet eigen Large Language Model met Retrieval Augmented Generation  - LLM met RAG\n",
    "\n",
    "Vandaag maken we een eigen chatbot mét kennis van lokale documenten. Hieronder staat een stappenplan om deze zelf te bouwen. Maak waar nodig gebruik van een al bestaande chatbot (chatGPT, Mistral, ...) om je code op punt te stellen. Als alles goed is heb je deze daarna (minder hard) nodig..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "- Creeer een lokale folder met 'local-rag'\n",
    "- Creëer een virtuele environment, activeer deze\n",
    "- Installeer chromaDB\n",
    "- installeer langchain tools, die heb je nodig om je text te splitten\n",
    "- installeer ollama\n",
    "- haal het Mistral model binnen - dit kan wel even duren. In de tussentijd kan je best al even verder kijken wat je nog moet doen.\n",
    "- haal het model binnen om tekst te embedden: nomic-embed-text\n",
    "- Laat ollama draaien met `ollama serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webserver\n",
    "Zet een (Flask of fastapi of andere) webserver op, die volgende routes ondersteunt:\n",
    "- op de /embed, POST: hier ga je, gebruik makend van een `embed` methode uit de `embed.py` file (zie verder) een document inbedden. Als dit is gelukt geef je een positieve response message terug. Voorzie basic error handling.\n",
    "- /query, POST: gebruik makend van de `query` methode uit de `query.py`, ga je hier een vraag inlezen.\n",
    "\n",
    "`jsonify, request, Flask` kunnen nuttige libraries zijn. In essentie is deze webserver gewoon een doorgeefluik voor de volgende codefiles.\n",
    "\n",
    "\n",
    "### Embedding\n",
    "**Embedding** in `embed.py` file: voorzie volgende methodes:\n",
    "- gebruik deze code snippet om enkel pdf's toe te staan:\n",
    "```python\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in {'pdf'}\n",
    "```\n",
    "\n",
    "- Gebruik deze code snippet om een geüploade file op te slagen in een tijdelijke folder:\n",
    "```python\n",
    "def save_file(file):\n",
    "    # Save the uploaded file with a secure filename and return the file path\n",
    "    ct = datetime.now()\n",
    "    ts = ct.timestamp()\n",
    "    filename = str(ts) + \"_\" + secure_filename(file.filename)\n",
    "    file_path = os.path.join(TEMP_FOLDER, filename)\n",
    "    file.save(file_path)\n",
    "\n",
    "    return file_path\n",
    "```\n",
    "\n",
    "- maak een methode `load_and_split_date(file_path)` die een datafile achtereenvolgens\n",
    "    - inlaadt met `UnstructuredPDFLoader`\n",
    "    - in stukken splits met een `RecursiveCharacterTextSplitter` - experimenteer met de chunk size en chunk_overlap. Dit bepaalt hoe groot de passages tekst zijn die in de database zullen worden opgeslagen.\n",
    "    - geef die chunks terug als return van deze methode.\n",
    "\n",
    "-  een methode `embed(file)`:  Check if the file is valid, save it, load and split the data, add to the database, and remove the temporary file\n",
    "    - check of de filename geldig is\n",
    "    - save de file\n",
    "    - laad deze in en split de data in chunks (met `load_and_split_data`)\n",
    "    - sla de data op in de vectordatabase (zie verder)\n",
    "    - delete de tijdelijke data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "\n",
    "gebruik deze code voor de vector database. Ze connecteert naadloos met de andere systemen:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = os.getenv('CHROMA_PATH', 'chroma')\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'local-rag')\n",
    "TEXT_EMBEDDING_MODEL = os.getenv('TEXT_EMBEDDING_MODEL', 'nomic-embed-text')\n",
    "\n",
    "def get_vector_db():\n",
    "    embedding = OllamaEmbeddings(model=TEXT_EMBEDDING_MODEL,show_progress=True)\n",
    "\n",
    "    db = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "    return db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "Tot slot heb je code nodig die op basis van een query extra context gaat zoeken, in de vector database, en deze informatie als context toevoegt aan een query. Implementeer dit op deze manier:\n",
    "```python\n",
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from get_vector_db import get_vector_db\n",
    "\n",
    "LLM_MODEL = os.getenv('LLM_MODEL', 'mistral')\n",
    "\n",
    "def get_prompt():\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"Hier kan je infor meegeven over hoe de chatbot zich moet gedragen {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    return QUERY_PROMPT, prompt\n",
    "\n",
    "# Main function to handle the query process\n",
    "def query(input):\n",
    "    if input:\n",
    "        # Initialize the language model with the specified model name\n",
    "        llm = ChatOllama(model=LLM_MODEL)\n",
    "        # Get the vector database instance\n",
    "        db = get_vector_db()\n",
    "        # Get the prompt templates\n",
    "        QUERY_PROMPT, prompt = get_prompt()\n",
    "\n",
    "        # Set up the retriever to generate multiple queries using the language model and the query prompt\n",
    "        retriever = MultiQueryRetriever.from_llm(\n",
    "            db.as_retriever(), \n",
    "            llm,\n",
    "            prompt=QUERY_PROMPT\n",
    "        )\n",
    "\n",
    "        # Define the processing chain to retrieve context, generate the answer, and parse the output\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        response = chain.invoke(input)\n",
    "\n",
    "        return response\n",
    "\n",
    "    return None\n",
    "\n",
    "```\n",
    "\n",
    "Wat nog ontbreekt is de inhoud van de `query(input)` prompt. Ga als volgt tewerk:\n",
    "- initialiseer een LLM met de geimporteerde chatOllama methode\n",
    "- gebruik de get_vector_db (hierboven gedefinieerd) om die te kunnen aanroepen\n",
    "- gebruik de prompt template\n",
    "- maak een `retriever`, gebruik de `MultiQueryRetriever.from_llm` methode, geef de llm, de database en de prompt mee\n",
    "- maak een `chain` om alle stukken samen te brengen:\n",
    "    - context halen met de retriever\n",
    "    - prompt gebruiken\n",
    "    - llm aanroepen\n",
    "    - output parsen met StrOutputParser\n",
    "- uiteindelijk kan je met `response = chain.invoke(input) ` en `return response` je antwoord teruggeven."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
