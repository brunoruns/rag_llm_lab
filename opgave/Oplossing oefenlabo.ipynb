{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opzet eigen Large Language Model met Retrieval Augmented Generation  - LLM met RAG\n",
    "\n",
    "Vandaag maken we een eigen chatbot mét kennis van lokale documenten. Hieronder staat een stappenplan om deze zelf te bouwen. Maak waar nodig gebruik van een al bestaande chatbot (chatGPT, Mistral, ...) om je code op punt te stellen. Als alles goed is heb je deze daarna (minder hard) nodig..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "- Creeer een lokale folder met 'local-rag'\n",
    "- Creëer een virtuele environment, activeer deze\n",
    "- Installeer chromaDB\n",
    "- installeer langchain tools, die heb je nodig om je text te splitten\n",
    "- installeer ollama\n",
    "- haal het Mistral model binnen - dit kan wel even duren. In de tussentijd kan je best al even verder kijken wat je nog moet doen.\n",
    "- haal het model binnen om tekst te embedden: nomic-embed-text\n",
    "- Laat ollama draaien met `ollama serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webserver\n",
    "Zet een (Flask of fastapi of andere) webserver op, die volgende routes ondersteunt:\n",
    "- op de /embed, POST: hier ga je, gebruik makend van een `embed` methode uit de `embed.py` file (zie verder) een document inbedden. Als dit is gelukt geef je een positieve response message terug. Voorzie basic error handling.\n",
    "- /query, POST: gebruik makend van de `query` methode uit de `query.py`, ga je hier een vraag inlezen.\n",
    "\n",
    "`jsonify, request, Flask` kunnen nuttige libraries zijn. In essentie is deze webserver gewoon een doorgeefluik voor de volgende codefiles.\n",
    "\n",
    "\n",
    "### Embedding\n",
    "**Embedding** in `embed.py` file: voorzie volgende methodes:\n",
    "- gebruik deze code snippet om enkel pdf's toe te staan:\n",
    "```python\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in {'pdf'}\n",
    "```\n",
    "\n",
    "- Gebruik deze code snippet om een geüploade file op te slagen in een tijdelijke folder:\n",
    "```python\n",
    "def save_file(file):\n",
    "    # Save the uploaded file with a secure filename and return the file path\n",
    "    ct = datetime.now()\n",
    "    ts = ct.timestamp()\n",
    "    filename = str(ts) + \"_\" + secure_filename(file.filename)\n",
    "    file_path = os.path.join(TEMP_FOLDER, filename)\n",
    "    file.save(file_path)\n",
    "\n",
    "    return file_path\n",
    "```\n",
    "\n",
    "- maak een methode `load_and_split_date(file_path)` die een datafile achtereenvolgens\n",
    "    - inlaadt met `UnstructuredPDFLoader`\n",
    "    - in stukken splits met een `RecursiveCharacterTextSplitter` - experimenteer met de chunk size en chunk_overlap. Dit bepaalt hoe groot de passages tekst zijn die in de database zullen worden opgeslagen.\n",
    "    - geef die chunks terug als return van deze methode.\n",
    "\n",
    "-  een methode `embed(file)`:  Check if the file is valid, save it, load and split the data, add to the database, and remove the temporary file\n",
    "    - check of de filename geldig is\n",
    "    - save de file\n",
    "    - laad deze in en split de data in chunks (met `load_and_split_data`)\n",
    "    - sla de data op in de vectordatabase (zie verder)\n",
    "    - delete de tijdelijke data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector database\n",
    "\n",
    "gebruik deze code voor de vector database. Ze connecteert naadloos met de andere systemen:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = os.getenv('CHROMA_PATH', 'chroma')\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'local-rag')\n",
    "TEXT_EMBEDDING_MODEL = os.getenv('TEXT_EMBEDDING_MODEL', 'nomic-embed-text')\n",
    "\n",
    "def get_vector_db():\n",
    "    embedding = OllamaEmbeddings(model=TEXT_EMBEDDING_MODEL,show_progress=True)\n",
    "\n",
    "    db = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "    return db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "Tot slot heb je code nodig die op basis van een query extra context gaat zoeken, in de vector database, en deze informatie als context toevoegt aan een query. Implementeer dit op deze manier:\n",
    "```python\n",
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from get_vector_db import get_vector_db\n",
    "\n",
    "LLM_MODEL = os.getenv('LLM_MODEL', 'mistral')\n",
    "\n",
    "def get_prompt():\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"Hier kan je infor meegeven over hoe de chatbot zich moet gedragen {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    return QUERY_PROMPT, prompt\n",
    "\n",
    "# Main function to handle the query process\n",
    "def query(input):\n",
    "    if input:\n",
    "        # Initialize the language model with the specified model name\n",
    "        llm = ChatOllama(model=LLM_MODEL)\n",
    "        # Get the vector database instance\n",
    "        db = get_vector_db()\n",
    "        # Get the prompt templates\n",
    "        QUERY_PROMPT, prompt = get_prompt()\n",
    "\n",
    "        # Set up the retriever to generate multiple queries using the language model and the query prompt\n",
    "        retriever = MultiQueryRetriever.from_llm(\n",
    "            db.as_retriever(), \n",
    "            llm,\n",
    "            prompt=QUERY_PROMPT\n",
    "        )\n",
    "\n",
    "        # Define the processing chain to retrieve context, generate the answer, and parse the output\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        response = chain.invoke(input)\n",
    "\n",
    "        return response\n",
    "\n",
    "    return None\n",
    "\n",
    "```\n",
    "\n",
    "Wat nog ontbreekt is de inhoud van de `query(input)` prompt. Ga als volgt tewerk:\n",
    "- initialiseer een LLM met de geimporteerde chatOllama methode\n",
    "- gebruik de get_vector_db (hierboven gedefinieerd) om die te kunnen aanroepen\n",
    "- gebruik de prompt template\n",
    "- maak een `retriever`, gebruik de `MultiQueryRetriever.from_llm` methode, geef de llm, de database en de prompt mee\n",
    "- maak een `chain` om alle stukken samen te brengen:\n",
    "    - context halen met de retriever\n",
    "    - prompt gebruiken\n",
    "    - llm aanroepen\n",
    "    - output parsen met StrOutputParser\n",
    "- uiteindelijk kan je met `response = chain.invoke(input) ` en `return response` je antwoord teruggeven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oplossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pip install --q chromadb`\n",
    "- `pip install --q unstructured langchain langchain-text-splitters`\n",
    "- `pip install --q \"unstructured[all-docs]`\n",
    "- `pip install --q langchain_community`\n",
    "- `pip install --q flask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama installation:\n",
    "- zie `setup-ollama.sh` en `install_ollama.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak een webserver via een flask:`\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from embed import embed\n",
    "from query import query\n",
    "from get_vector_db import get_vector_db\n",
    "\n",
    "TEMP_FOLDER = os.getenv('TEMP_FOLDER', './_temp')\n",
    "os.makedirs(TEMP_FOLDER, exist_ok=True)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/embed', methods=['POST'])\n",
    "def route_embed():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"No file part\"}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "\n",
    "    if file.filename == '':\n",
    "        return jsonify({\"error\": \"No selected file\"}), 400\n",
    "\n",
    "    embedded = embed(file)\n",
    "\n",
    "    if embedded:\n",
    "        return jsonify({\"message\": \"File embedded successfully\"}), 200\n",
    "\n",
    "    return jsonify({\"error\": \"File embedded unsuccessfully\"}), 400\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def route_query():\n",
    "    data = request.get_json()\n",
    "    response = query(data.get('query'))\n",
    "\n",
    "    if response:\n",
    "        return jsonify({\"message\": response}), 200\n",
    "\n",
    "    return jsonify({\"error\": \"Something went wrong\"}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De embedding heeft als doel om bestanden naar de vector database te brengen zodat hier efficiënt kan worden opgezocht. De steek je in een file `embed.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from werkzeug.utils import secure_filename\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from get_vector_db import get_vector_db\n",
    "\n",
    "TEMP_FOLDER = os.getenv('TEMP_FOLDER', './_temp')\n",
    "\n",
    "# Function to check if the uploaded file is allowed (only PDF files)\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in {'pdf'}\n",
    "\n",
    "# Function to save the uploaded file to the temporary folder\n",
    "def save_file(file):\n",
    "    # Save the uploaded file with a secure filename and return the file path\n",
    "    ct = datetime.now()\n",
    "    ts = ct.timestamp()\n",
    "    filename = str(ts) + \"_\" + secure_filename(file.filename)\n",
    "    file_path = os.path.join(TEMP_FOLDER, filename)\n",
    "    file.save(file_path)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "# Function to load and split the data from the PDF file\n",
    "def load_and_split_data(file_path):\n",
    "    # Load the PDF file and split the data into chunks\n",
    "    loader = UnstructuredPDFLoader(file_path=file_path)\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Main function to handle the embedding process\n",
    "def embed(file):\n",
    "    # Check if the file is valid, save it, load and split the data, add to the database, and remove the temporary file\n",
    "    if file.filename != '' and file and allowed_file(file.filename):\n",
    "        file_path = save_file(file)\n",
    "        chunks = load_and_split_data(file_path)\n",
    "        db = get_vector_db()\n",
    "        db.add_documents(chunks)\n",
    "        db.persist()\n",
    "        os.remove(file_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De query van de gebruiker verloopt via deze code. Let op de globale structuur van de code:\n",
    "- er wordt een template voorzien met specifieke context\n",
    "- er wordt een MultiQueryRetriever gebruikt om meerdere antwoorden te genereren. \n",
    "- er wordt een antwoord gegeven rekening houdend met de context, die in de vector database te vinden is\n",
    "\n",
    "`query.py` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from get_vector_db import get_vector_db\n",
    "\n",
    "LLM_MODEL = os.getenv('LLM_MODEL', 'mistral')\n",
    "\n",
    "# Function to get the prompt templates for generating alternative questions and answering based on context\n",
    "def get_prompt():\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "        different versions of the given user question to retrieve relevant documents from\n",
    "        a vector database. By generating multiple perspectives on the user question, your\n",
    "        goal is to help the user overcome some of the limitations of the distance-based\n",
    "        similarity search. Provide these alternative questions separated by newlines.\n",
    "        Original question: {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    return QUERY_PROMPT, prompt\n",
    "\n",
    "# Main function to handle the query process\n",
    "def query(input):\n",
    "    if input:\n",
    "        # Initialize the language model with the specified model name\n",
    "        llm = ChatOllama(model=LLM_MODEL)\n",
    "        # Get the vector database instance\n",
    "        db = get_vector_db()\n",
    "        # Get the prompt templates\n",
    "        QUERY_PROMPT, prompt = get_prompt()\n",
    "\n",
    "        # Set up the retriever to generate multiple queries using the language model and the query prompt\n",
    "        retriever = MultiQueryRetriever.from_llm(\n",
    "            db.as_retriever(), \n",
    "            llm,\n",
    "            prompt=QUERY_PROMPT\n",
    "        )\n",
    "\n",
    "        # Define the processing chain to retrieve context, generate the answer, and parse the output\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        response = chain.invoke(input)\n",
    "\n",
    "        return response\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De vector database wordt aanroepen via deze methode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = os.getenv('CHROMA_PATH', 'chroma')\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'local-rag')\n",
    "TEXT_EMBEDDING_MODEL = os.getenv('TEXT_EMBEDDING_MODEL', 'nomic-embed-text')\n",
    "\n",
    "def get_vector_db():\n",
    "    embedding = OllamaEmbeddings(model=TEXT_EMBEDDING_MODEL,show_progress=True)\n",
    "\n",
    "    db = Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "    return db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als laatste hebben we nog wat environment variabelen nodig in een `.env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TEMP_FOLDER = './_temp'\n",
    "CHROMA_PATH = 'chroma'\n",
    "COLLECTION_NAME = 'local-rag'\n",
    "LLM_MODEL = 'mistral'\n",
    "TEXT_EMBEDDING_MODEL = 'nomic-embed-text'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server laten draaien\n",
    "De server laten draaien doe je met `python app.py`. Nu kan je dan requests sturen, bijvoorbeeld een CV uploaden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "curl --request POST \\\n",
    "--url http://localhost:8080/embed \\\n",
    "--header 'Content-Type: multipart/form-data' \\\n",
    "--form file=@/PATHTOFILE/CV.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan kan je vragen stellen over dit CV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl --request POST \\\n",
    "  --url http://localhost:8080/query \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --data '{ \"query\": \"Wat heeft -- gestudeerd ?\" }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
